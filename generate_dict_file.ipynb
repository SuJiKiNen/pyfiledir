{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Create pinyin data from Unihan kHanyuPinlu,kHanyuPinyin,kXHC1983 data,\n",
    "This may not contains all GB18030 characters,but it cover most used.\n",
    "and beyond this,this implemention given possbility to support Japanese and Korean.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "archive = zipfile.ZipFile('data/Unihan.zip', 'r')\n",
    "reading_file = archive.open('Unihan_Readings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(reading_file, sep='\\t', comment='#', names=['char', 'field', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_parser(s: str) -> str:\n",
    "    return chr(int(s[-4:], 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kHanyuPinyin_parser(s: str) -> list:\n",
    "    \"\"\"\n",
    "    >>> kHanyuPinyin_parser(\"10009.010:jī,qí\")\n",
    "    ['jī', 'qí']\n",
    "    >>> kHanyuPinyin_parser(\"31641.040:hán,gàn 80023.120:gàn,hán\")\n",
    "    ['hán', 'gàn', 'gàn', 'hán']\n",
    "    \"\"\"\n",
    "    blocks = s.split(' ')\n",
    "    ret = []\n",
    "    for b in blocks:\n",
    "        ret.extend(b.split(':')[1].split(','))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kHanyuPinlu_parser(s: str) -> list:\n",
    "    \"\"\"\n",
    "    >>> kHanyuPinlu_parser(\"shàng(12308) shang(392)\")\n",
    "    ['shàng', 'shang']\n",
    "    \"\"\"\n",
    "    def keep_pinyin(s):\n",
    "        \"\"\"remove digits and ( and )\"\"\"\n",
    "        return re.sub('[0-9\\(\\)]', '', s)\n",
    "    return list(map(keep_pinyin, s.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kXHC1983_parser(s: str) -> list:\n",
    "    \"\"\"\n",
    "    >>> kXHC1983_parser(\"k0811.021:ň  0826.021:ňg\")\n",
    "    ['ň', 'ňg']\n",
    "    \"\"\"\n",
    "    return [part.split(':')[1] for part in s.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kCantonese',\n",
       " 'kDefinition',\n",
       " 'kHangul',\n",
       " 'kHanyuPinlu',\n",
       " 'kHanyuPinyin',\n",
       " 'kJapaneseKun',\n",
       " 'kJapaneseOn',\n",
       " 'kKorean',\n",
       " 'kMandarin',\n",
       " 'kTang',\n",
       " 'kVietnamese',\n",
       " 'kXHC1983'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['field'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "for field in set(df['field']):\n",
    "    parser = field + \"_parser\"\n",
    "    if parser in globals().keys():\n",
    "        output[field] = defaultdict(list)\n",
    "        field_df = df[df['field'] == field]\n",
    "        parser_fn = globals()[parser]\n",
    "        for idx, row in field_df.iterrows():\n",
    "            char = char_parser(row['char'])\n",
    "            pinyin = parser_fn(row['value'])\n",
    "            output[field][char].extend(pinyin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_dict = defaultdict(set)\n",
    "for field in output.keys():\n",
    "    for char in output[field].keys():\n",
    "        py_dict[char].update(output[field][char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "pure_py_dict = {}\n",
    "def remove_accent(s: str) -> str:\n",
    "    \"\"\"\n",
    "    >>> remove_accent(\"wā\")\n",
    "    'wa'\n",
    "    \"\"\"\n",
    "    return (\n",
    "        unicodedata\n",
    "        .normalize('NFKD', s)\n",
    "        .encode('ascii','ignore')\n",
    "        .decode()\n",
    "    )\n",
    "\n",
    "for key in py_dict.keys():\n",
    "    pure_py_dict[key] = list(sorted(set(map(remove_accent, py_dict[key]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sep = os.linesep\n",
    "with open('pyfiledir/py_dict.py','w', encoding=\"utf-8\") as f:\n",
    "    print(\"#!/usr/bin/env python\",  end=sep, file=f)\n",
    "    print(\"# -*- coding: utf-8 -*-\",  end=sep, file=f)\n",
    "    print('\"\"\"This file was auto-generated by pyfiledir script.\"\"\"', end=sep, file=f)\n",
    "    print(\"\",  end=os.linesep, file=f)\n",
    "    print(\"PY_DICT = {\", end=sep, file=f)\n",
    "    for char in pure_py_dict.keys():\n",
    "        try:\n",
    "            char.encode(\"ascii\")\n",
    "        except UnicodeEncodeError:\n",
    "            line = ' ' * 4 + \"{}: {},\".format(repr(char), repr(pure_py_dict[char]))\n",
    "            print(line, end=sep, file=f)\n",
    "    f.write(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfiledir.py_core import GB2312EncodeingRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['盕', '盙', '盫', '盽', '県', '睓', '瞆', '瞓', '瞾', '矁', '矅', '矋',\n",
      "'矏', '矒', '矝', '矤', '矦', '砇', '砕', '砙', '砤', '砿', '硂', '硓', '硛',\n",
      "'硲', '硳', '碈', '碯', '碷', '磇', '磗', '磘', '磮', '磰', '磱', '礀', '礂',\n",
      "'礈', '礝', '礢', '礶', '礸', '祂', '祅', '祙', '祬', '祶', '禆', '禇', '禉',\n",
      "'禙', '禣', '禥', '禴', '禵', '秐', '秗', '秮', '秼', '稁', '稇', '稤', '稥',\n",
      "'稺', '穂', '穌', '穏', '穞', '穯', '窂', '窛', '窧', '窴', '窽', '竂', '竃',\n",
      "'竆', '竍', '竏', '竐', '竔', '竩', '竰', '竸', '竼', '笟', '笶', '笹', '笻',\n",
      "'笽', '筂', '筙', '筨', '筺', '筿', '箏', '箚', '篐', '篭', '簓', '簔', '簗',\n",
      "'簤', '簮', '簼', '籂', '籎', '籕', '籖', '籗', '籭', '籶', '籿', '粁', '粊',\n",
      "'粎', '粏', '粐', '粖', '粚', '粛', '粠', '粩', '粫', '糓', '糘', '糤', '糥',\n",
      "'糳', '紦', '絵', '絶', '綂', '綗', '綘', '続', '綛', '綤', '綳', '緓', '緕',\n",
      "'緖', '緫', '緮', '緽', '縀', '縁', '縃', '縄', '縇', '繍', '繤', '繧', '繱',\n",
      "'繿', '纄', '纉', '绬', '缷', '缼', '罀', '罙', '羀', '羃', '羪', '翑', '翤',\n",
      "'翭', '耂', '耉', '耊', '聓', '聜', '聢', '聣', '聦', '聨', '聫', '聮', '聴',\n",
      "'肔', '肻', '胐', '胑', '胢', '脳', '脵', '腁', '膐', '膓', '膖', '膤', '膥',\n",
      "'膶', '臋', '臓', '臰', '舃', '舑', '舤', '舿', '艈', '艊', '艔', '艠', '艶',\n",
      "'芁', '芌', '芕', '芲', '苂', '苆', '苿', '茊', '茋', '茐', '茒', '荕', '荘',\n",
      "'荝', '莣', '莵', '莻', '菦', '菷', '萂', '萅', '萗', '萙', '萞', '萟', '萠',\n",
      "'萢', '葘', '葢', '蒏', '蒕', '蒭', '蓙', '蓛', '蓜', '蓤', '蓾', '蔅', '蔆',\n",
      "'蔉', '蔋', '蔐', '蔒', '蔲', '蔳', '蔶', '蕌', '蕏', '蕐', '蕯', '蕱', '蕳',\n",
      "'蕵', '薀', '薗', '薫', '薻', '藌', '藔', '藮', '藲', '藳', '藵', '蘍', '蘎',\n",
      "'蘏', '蘒', '蘓', '蘕', '蘝', '蘤', '蘨', '蘯', '蘰', '蘷', '虅', '虉', '虊',\n",
      "'虌', '虗', '虝', '蚃', '蚉', '蛠', '蛻', '蛽', '蛿', '蜝', '蜫', '蝅', '蝊',\n",
      "'蝿', '螁', '螡', '螥', '螦', '螧', '蟌', '蟐', '蟩', '蟰', '蠝', '蠫', '蠯',\n",
      "'蠴', '蠺', '衜', '衟', '袡', '袰', '裭', '褀', '褁', '褅', '褌', '褜', '襅',\n",
      "'襔', '襥', '襩', '襷', '襽', '覀', '覉', '覌', '覍', '覐', '覚', '覧', '觹',\n",
      "'觼', '觽', '訁', '訫', '訮', '訽', '訿', '詋', '誝', '誟', '誫', '読', '誮',\n",
      "'誯', '諊', '諙', '账', '諩', '諬', '謃', '謢', '謥', '謩', '謭', '謸', '譋',\n",
      "'譐', '譡', '譢', '譪', '譱', '譲', '譵', '譻', '譼', '譿', '讃', '讍', '谉']\n"
     ]
    }
   ],
   "source": [
    "uncovered_chars = []\n",
    "_st_num = int.from_bytes(GB2312EncodeingRange.min_codepoint, byteorder=\"big\")\n",
    "_ed_num = int.from_bytes(GB2312EncodeingRange.max_codepoint, byteorder=\"big\")\n",
    "for num in range(_st_num, _ed_num+1):\n",
    "    char_bytes = num.to_bytes(length=2, byteorder=\"big\")\n",
    "    try:\n",
    "        char = char_bytes.decode(\"GB18030\")\n",
    "        if char not in pure_py_dict.keys():\n",
    "            uncovered_chars.append(char)\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "from textwrap import fill\n",
    "chars_list = [x for x in uncovered_chars ]\n",
    "print(fill(str(chars_list), width=64) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import doctest\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
